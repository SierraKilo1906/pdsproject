{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Electricity Demand in Pittsburgh, PA\n",
    "\n",
    "In this project, we will attempt to forecast Pittsburgh electrical demand through a combination of heuristic knowledge and historic demand data. Based on the assumption that this is a regression problem, our aim is to compare commonly used regression techniques using the same feature set. \n",
    "\n",
    "How much does the use of ensemble techniques matter when prediction quality and accuracy is concerned? How generalizable is the model we use? There are some questions we hope to answer through this exploratory project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Motivation](#Motivation)\n",
    "* [Packages and Dependencies](#Packages)\n",
    "* [Data Sources and Collection](#DataCollection)\n",
    "    * [Electric Demand Data](#elecdata)\n",
    "    * [Weather Data](#weatherdata)\n",
    "* [Data Aggregation](#dataagg)\n",
    "* [Preliminary Data Visualization](#datavis)\n",
    "* [Type Conversions and Pre-processing](#preprocess)\n",
    "* [Preparing input data for Regression Methods](#tonumpy)\n",
    "* [Feature Selection using Orthogonal Matching Pursuit](#omp)\n",
    "* [Analysis - Simple Regression, Random Forest Regression, Bagging Regression](#regression)\n",
    "    * [Preliminary Analysis](#prelim)\n",
    "    * [Model Comparison](#comparison)\n",
    "* [Comparing Forecast with PJM Forecast](#forecastcomparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation - Why predict energy demand? <a name = \"Motivation\"></a>\n",
    "\n",
    "Conservation of energy and energy management for both environmental and economic factors is an age old problem. Using predicted energy demand, utilities can manage generation and peak load shaving techniques. However, inaccurate predictions can lead to surplus generation, or worse still, energy deficits. Additionally, the advent of renewable energy generation has led to application of hourly load predictions for demand response programs and associated monetary transactions. Energy demand prediction has implications in the renewable energy sector, as well as for energy infrastructure system development, such as the design or planning of microgrids with the availability of more granular, neighborhood specific data. \n",
    "\n",
    "For these reasons, we explore the difference in accuracy as a result of using different regression methods. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages and Dependencies <a name = \"Packages\"></a>\n",
    "\n",
    "The various packages required to be installed for this project are listed here. \n",
    "\n",
    "#### For weather data collection\n",
    "* PyPi package wwo-hist\n",
    "\n",
    "#### For data preprocessing and visualization \n",
    "* pandas\n",
    "* glob\n",
    "* seaborn\n",
    "* matplotlib\n",
    "\n",
    "#### For regression models\n",
    "* scikitlearn - sklearn\n",
    "\n",
    "Each of these packages is imported below, and will be used to execute the various steps of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather\n",
    "from wwo_hist import retrieve_hist_data\n",
    "#data processing and visualization\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "\n",
    "import collections\n",
    "import requests\n",
    "import json\n",
    "import time \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources and Collection <a name=\"DataCollection\"></a>\n",
    "\n",
    "To predict future electrical demand, two sources of data were considered most important -\n",
    "\n",
    "1) Historic Electric Demand Data\n",
    "2) Historic Weather Data\n",
    "\n",
    "### Historic Electric Demand Data <a name = \"elecdata\"></a>\n",
    "\n",
    "Historic Electric Demand Data for Pittsburgh is openly published by [PJM](https://www.pjm.com/markets-and-operations/etools/data-miner-2.aspx), the Regional Transmission Organization that oversees the city's electricity management. This data can be obtained programmatically, or downloaded as a CSV file. To gain access to developer tools or the API, one must be added to an authorized organization. However, the CSV download is free for all. For this reason, the project utilizes CSV data that from PJM's [DataMiner2 Interface](https://dataminer2.pjm.com/feed/hrl_load_metered). \n",
    "Using this interface, data was collected for a duration of three years, from May 1st 2016 to April 30th 2019. \n",
    "\n",
    "Yearly records were obtained in CSV format and are read in using the function `get_electric_load_city` below. A dataframe containing the historic demand data is also initialized here using the dataframe returned by this function. We can see that there are 26280 hourly instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26280, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime_beginning_ept</th>\n",
       "      <th>datetime_beginning_utc</th>\n",
       "      <th>is_verified</th>\n",
       "      <th>load_area</th>\n",
       "      <th>mkt_region</th>\n",
       "      <th>mw</th>\n",
       "      <th>nerc_region</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/1/2016 0:00</td>\n",
       "      <td>5/1/2016 4:00</td>\n",
       "      <td>True</td>\n",
       "      <td>DUQ</td>\n",
       "      <td>WEST</td>\n",
       "      <td>1138.573</td>\n",
       "      <td>RFC</td>\n",
       "      <td>DUQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5/1/2016 1:00</td>\n",
       "      <td>5/1/2016 5:00</td>\n",
       "      <td>True</td>\n",
       "      <td>DUQ</td>\n",
       "      <td>WEST</td>\n",
       "      <td>1089.713</td>\n",
       "      <td>RFC</td>\n",
       "      <td>DUQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5/1/2016 2:00</td>\n",
       "      <td>5/1/2016 6:00</td>\n",
       "      <td>True</td>\n",
       "      <td>DUQ</td>\n",
       "      <td>WEST</td>\n",
       "      <td>1075.268</td>\n",
       "      <td>RFC</td>\n",
       "      <td>DUQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/1/2016 3:00</td>\n",
       "      <td>5/1/2016 7:00</td>\n",
       "      <td>True</td>\n",
       "      <td>DUQ</td>\n",
       "      <td>WEST</td>\n",
       "      <td>1068.282</td>\n",
       "      <td>RFC</td>\n",
       "      <td>DUQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5/1/2016 4:00</td>\n",
       "      <td>5/1/2016 8:00</td>\n",
       "      <td>True</td>\n",
       "      <td>DUQ</td>\n",
       "      <td>WEST</td>\n",
       "      <td>1062.133</td>\n",
       "      <td>RFC</td>\n",
       "      <td>DUQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datetime_beginning_ept datetime_beginning_utc  is_verified load_area  \\\n",
       "0          5/1/2016 0:00          5/1/2016 4:00         True       DUQ   \n",
       "1          5/1/2016 1:00          5/1/2016 5:00         True       DUQ   \n",
       "2          5/1/2016 2:00          5/1/2016 6:00         True       DUQ   \n",
       "3          5/1/2016 3:00          5/1/2016 7:00         True       DUQ   \n",
       "4          5/1/2016 4:00          5/1/2016 8:00         True       DUQ   \n",
       "\n",
       "  mkt_region        mw nerc_region zone  \n",
       "0       WEST  1138.573         RFC  DUQ  \n",
       "1       WEST  1089.713         RFC  DUQ  \n",
       "2       WEST  1075.268         RFC  DUQ  \n",
       "3       WEST  1068.282         RFC  DUQ  \n",
       "4       WEST  1062.133         RFC  DUQ  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returns dataframe of electric load data from csvs\n",
    "#city can be defined based on stored directory\n",
    "#files named starting with 'hrl_load_metered'\n",
    "def get_electric_load_city(dirname = os.getcwd()):\n",
    "    path = os.path.join(dirname, 'hrl_load_metered_')\n",
    "    all_files = glob.glob(path + \"*.csv\")\n",
    "    \n",
    "    csv_list = []\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, header = 0)\n",
    "        csv_list.append(df)\n",
    "        \n",
    "    electric_df = pd.concat(csv_list, axis = 0, ignore_index = True, sort = True)\n",
    "    \n",
    "    return electric_df\n",
    "\n",
    "electric_df = get_electric_load_city()\n",
    "print(electric_df.shape)\n",
    "#printing out header rows to check columns created\n",
    "electric_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Historic Weather Data <a name = \"weatherdata\"></a>\n",
    "\n",
    "Weather data to be used for this prediction was obtained programmatically using PyPi package **wwo-hist**. wwo-hist uses data from [World Weather Online](https://www.worldweatheronline.com/developer/login.aspx?ReturnUrl=%2fdeveloper%2fmy%2f).  Documentation for the wwo-hist package can be found at https://pypi.org/project/wwo-hist/. \n",
    "\n",
    "To try this out, get a free API key (no billing information needed!) [here](https://www.worldweatheronline.com/developer/) and store it in a `.txt` file in your notebook directory. The function below reads the API, to use for the query. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using method from hw1 to hide api key\n",
    "def read_api_key(filepath = \"weather_api_key.txt\"):\n",
    "    with open(filepath) as f:\n",
    "        lines = f.read()\n",
    "        \n",
    "    return lines.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can get the API key, we use it to get historic weather data using the API and wwo-hist to get hourly weather data for the same duration as electric demand data, May 1st 2016 to May 1st 2019. Since these are hourly values and the end time cannot vary in the definition, the extra day will be dropped later. \n",
    "\n",
    "The function below returns a wwo-hist stored \"dataframe\", contained in a list where the first element is a dataframe. By setting the `export_csv` parameter to True in the `retrieve_hist_data` method, a csv file containing the same data as the dataframe is stored. This can be used to reduce the number of calls to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Retrieving weather data for pittsburgh\n",
      "\n",
      "\n",
      "Currently retrieving data for pittsburgh: from 2016-05-01 to 2016-05-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:00.897640\n",
      "Currently retrieving data for pittsburgh: from 2016-06-01 to 2016-06-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:03.029548\n",
      "Currently retrieving data for pittsburgh: from 2016-07-01 to 2016-07-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:03.983769\n",
      "Currently retrieving data for pittsburgh: from 2016-08-01 to 2016-08-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:06.294674\n",
      "Currently retrieving data for pittsburgh: from 2016-09-01 to 2016-09-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:08.625716\n",
      "Currently retrieving data for pittsburgh: from 2016-10-01 to 2016-10-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:10.741868\n",
      "Currently retrieving data for pittsburgh: from 2016-11-01 to 2016-11-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:12.686192\n",
      "Currently retrieving data for pittsburgh: from 2016-12-01 to 2016-12-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:15.887904\n",
      "Currently retrieving data for pittsburgh: from 2017-01-01 to 2017-01-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:18.224088\n",
      "Currently retrieving data for pittsburgh: from 2017-02-01 to 2017-02-28\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:20.415349\n",
      "Currently retrieving data for pittsburgh: from 2017-03-01 to 2017-03-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:22.607047\n",
      "Currently retrieving data for pittsburgh: from 2017-04-01 to 2017-04-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:24.696515\n",
      "Currently retrieving data for pittsburgh: from 2017-05-01 to 2017-05-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:27.122025\n",
      "Currently retrieving data for pittsburgh: from 2017-06-01 to 2017-06-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:29.822263\n",
      "Currently retrieving data for pittsburgh: from 2017-07-01 to 2017-07-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:35.258303\n",
      "Currently retrieving data for pittsburgh: from 2017-08-01 to 2017-08-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:36.309897\n",
      "Currently retrieving data for pittsburgh: from 2017-09-01 to 2017-09-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:38.402066\n",
      "Currently retrieving data for pittsburgh: from 2017-10-01 to 2017-10-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:39.496652\n",
      "Currently retrieving data for pittsburgh: from 2017-11-01 to 2017-11-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:40.317553\n",
      "Currently retrieving data for pittsburgh: from 2017-12-01 to 2017-12-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:41.206780\n",
      "Currently retrieving data for pittsburgh: from 2018-01-01 to 2018-01-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:41.839990\n",
      "Currently retrieving data for pittsburgh: from 2018-02-01 to 2018-02-28\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:42.408466\n",
      "Currently retrieving data for pittsburgh: from 2018-03-01 to 2018-03-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:43.036788\n",
      "Currently retrieving data for pittsburgh: from 2018-04-01 to 2018-04-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:43.681662\n",
      "Currently retrieving data for pittsburgh: from 2018-05-01 to 2018-05-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:44.373811\n",
      "Currently retrieving data for pittsburgh: from 2018-06-01 to 2018-06-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:44.989252\n",
      "Currently retrieving data for pittsburgh: from 2018-07-01 to 2018-07-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:45.677391\n",
      "Currently retrieving data for pittsburgh: from 2018-08-01 to 2018-08-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:46.343543\n",
      "Currently retrieving data for pittsburgh: from 2018-09-01 to 2018-09-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:47.025718\n",
      "Currently retrieving data for pittsburgh: from 2018-10-01 to 2018-10-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:48.803142\n",
      "Currently retrieving data for pittsburgh: from 2018-11-01 to 2018-11-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:50.987935\n",
      "Currently retrieving data for pittsburgh: from 2018-12-01 to 2018-12-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:51.773826\n",
      "Currently retrieving data for pittsburgh: from 2019-01-01 to 2019-01-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:53.661652\n",
      "Currently retrieving data for pittsburgh: from 2019-02-01 to 2019-02-28\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:55.371881\n",
      "Currently retrieving data for pittsburgh: from 2019-03-01 to 2019-03-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:56.091219\n",
      "Currently retrieving data for pittsburgh: from 2019-04-01 to 2019-04-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:57.906501\n",
      "Currently retrieving data for pittsburgh: from 2019-05-01 to 2019-05-01\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:58.054110\n",
      "\n",
      "\n",
      "export pittsburgh completed!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#function to get weather data using wwo-hist package\n",
    "#package code reference https://pypi.org/project/wwo-hist/\n",
    "def get_wwohist_weather(location, start_date, end_date, frequency, api_key = read_api_key()):\n",
    "    hist_weather = retrieve_hist_data(api_key,\n",
    "                                  location,\n",
    "                                  start_date,\n",
    "                                  end_date,\n",
    "                                  frequency,\n",
    "                                  export_csv = True,\n",
    "                                  store_df = True)\n",
    "    return hist_weather\n",
    "\n",
    "#weather data\n",
    "hist_weather = get_wwohist_weather(['pittsburgh'], '01-MAY-2016','01-MAY-2019',1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "\n",
    "#use this when API query is used\n",
    "weather_df = hist_weather[0]\n",
    "\n",
    "# uncomment lines below to use once first query is completed. \n",
    "\n",
    "# weather_df = pd.read_csv(\"pittsburgh.csv\")\n",
    "# weather_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aggregation <a name = \"dataagg\"></a>\n",
    "\n",
    "Now that we have both data sources, we combine the sources so that hourly values for load and weather condtions are aligned. The function below is used to combine the two dataframes, and some columns that were immediately apparent to be irrelevant to the prediction problem were dropped in this process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26280, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mw</th>\n",
       "      <th>maxtempC</th>\n",
       "      <th>mintempC</th>\n",
       "      <th>totalSnow_cm</th>\n",
       "      <th>sunHour</th>\n",
       "      <th>uvIndex</th>\n",
       "      <th>uvIndex</th>\n",
       "      <th>moon_illumination</th>\n",
       "      <th>moonrise</th>\n",
       "      <th>moonset</th>\n",
       "      <th>...</th>\n",
       "      <th>HeatIndexC</th>\n",
       "      <th>WindChillC</th>\n",
       "      <th>WindGustKmph</th>\n",
       "      <th>cloudcover</th>\n",
       "      <th>humidity</th>\n",
       "      <th>precipMM</th>\n",
       "      <th>pressure</th>\n",
       "      <th>tempC</th>\n",
       "      <th>winddirDegree</th>\n",
       "      <th>windspeedKmph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-05-01 00:00:00</th>\n",
       "      <td>1138.573</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>02:09 AM</td>\n",
       "      <td>01:22 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>100</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1014</td>\n",
       "      <td>12</td>\n",
       "      <td>164</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-01 01:00:00</th>\n",
       "      <td>1089.713</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>02:09 AM</td>\n",
       "      <td>01:22 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>100</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1014</td>\n",
       "      <td>12</td>\n",
       "      <td>172</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-01 02:00:00</th>\n",
       "      <td>1075.268</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>02:09 AM</td>\n",
       "      <td>01:22 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>100</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1014</td>\n",
       "      <td>13</td>\n",
       "      <td>180</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-01 03:00:00</th>\n",
       "      <td>1068.282</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>02:09 AM</td>\n",
       "      <td>01:22 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>100</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013</td>\n",
       "      <td>14</td>\n",
       "      <td>188</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-01 04:00:00</th>\n",
       "      <td>1062.133</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>02:09 AM</td>\n",
       "      <td>01:22 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>100</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013</td>\n",
       "      <td>14</td>\n",
       "      <td>195</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           mw maxtempC mintempC totalSnow_cm sunHour uvIndex  \\\n",
       "2016-05-01 00:00:00  1138.573       20       11          0.0    11.0       1   \n",
       "2016-05-01 01:00:00  1089.713       20       11          0.0    11.0       1   \n",
       "2016-05-01 02:00:00  1075.268       20       11          0.0    11.0       1   \n",
       "2016-05-01 03:00:00  1068.282       20       11          0.0    11.0       1   \n",
       "2016-05-01 04:00:00  1062.133       20       11          0.0    11.0       1   \n",
       "\n",
       "                    uvIndex moon_illumination  moonrise   moonset  ...  \\\n",
       "2016-05-01 00:00:00       1                29  02:09 AM  01:22 PM  ...   \n",
       "2016-05-01 01:00:00       1                29  02:09 AM  01:22 PM  ...   \n",
       "2016-05-01 02:00:00       1                29  02:09 AM  01:22 PM  ...   \n",
       "2016-05-01 03:00:00       1                29  02:09 AM  01:22 PM  ...   \n",
       "2016-05-01 04:00:00       1                29  02:09 AM  01:22 PM  ...   \n",
       "\n",
       "                    HeatIndexC WindChillC WindGustKmph cloudcover humidity  \\\n",
       "2016-05-01 00:00:00         12         10           19        100       98   \n",
       "2016-05-01 01:00:00         12         11           21        100       98   \n",
       "2016-05-01 02:00:00         13         12           23        100       98   \n",
       "2016-05-01 03:00:00         14         12           24        100       98   \n",
       "2016-05-01 04:00:00         14         13           22        100       98   \n",
       "\n",
       "                    precipMM pressure tempC winddirDegree windspeedKmph  \n",
       "2016-05-01 00:00:00      0.0     1014    12           164            12  \n",
       "2016-05-01 01:00:00      0.0     1014    12           172            13  \n",
       "2016-05-01 02:00:00      0.0     1014    13           180            14  \n",
       "2016-05-01 03:00:00      0.0     1013    14           188            15  \n",
       "2016-05-01 04:00:00      0.0     1013    14           195            14  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_merged_load_weather(electric_df, weather_df):\n",
    "    #converting date-time column to datetime object for east of manipulation\n",
    "    electric_df[\"datetime_beginning_ept\"] = electric_df[\"datetime_beginning_ept\"].astype('datetime64[ns]')\n",
    "    #setting index\n",
    "    data = electric_df.set_index(\"datetime_beginning_ept\").join(weather_df.set_index(\"date_time\"))\n",
    "    #removing columns that are clearly not relevant to the problem statement\n",
    "    data = data.drop(columns = ['nerc_region', 'mkt_region', 'zone', 'load_area', \n",
    "                                'is_verified', 'datetime_beginning_utc', 'FeelsLikeC', 'visibility'])\n",
    "    return data\n",
    "\n",
    "#print data to check merge works\n",
    "data = get_merged_load_weather(electric_df, weather_df)\n",
    "print(data.shape)\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Data Visualization <a name = \"datavis\"></a>\n",
    "\n",
    "From heuristic knowledge, we expected that temperature variables would be associated with the demand. To assess what other variables had an apparent relationship with the outcome variable (demand) and the type of relationship, pairwise visualizations were used. The three functions defined below were used for this purpose for different subsets of data, as applicable. \n",
    "\n",
    "The feature variables collected can be classified as two types - \n",
    "\n",
    "1) Features that might vary every hour of every day (mw demand, dew point temperature, cloud cover, windspeed etc..)\n",
    "\n",
    "2) Features that are constant for a given day (max temperature, minimum temperature, daylight hours)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-963a7e9427d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mvis_pairgrids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_data_day\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-963a7e9427d8>\u001b[0m in \u001b[0;36mvis_pairgrids\u001b[1;34m(data_day)\u001b[0m\n\u001b[0;32m     28\u001b[0m     g = sns.PairGrid(data_day,\n\u001b[0;32m     29\u001b[0m                      \u001b[0mx_vars\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdata_day\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'mw'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                      y_vars=['mw'])\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\axisgrid.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, hue, hue_order, palette, hue_kws, vars, x_vars, y_vars, diag_sharey, height, aspect, despine, dropna, size)\u001b[0m\n\u001b[0;32m   1279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m         \u001b[1;31m# Label the axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1281\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_axis_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m         \u001b[1;31m# Sort out the hue variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\axisgrid.py\u001b[0m in \u001b[0;36m_add_axis_labels\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1518\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m             \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1520\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m             \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 1 with size 0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 0x180 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#returns data grouped by day\n",
    "def get_data_day(data):\n",
    "    return data.groupby([pd.Grouper(freq='D')], as_index=False).agg(['mean'])\n",
    "    \n",
    "#takes data grouped by day and returns visualization as pairwise matrix\n",
    "def vis_corr_matrix(data_day):\n",
    "    \n",
    "    #code referenced from https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "    sns.set(style=\"white\")\n",
    "    corr = data_day.corr()\n",
    "    corr = corr.round(2)\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(20, 20))\n",
    "    ax.set_title('Feature Correlation: Daily View')\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "                annot = True, annot_kws = {\"size\": 12} ,square=True, \n",
    "                linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    \n",
    "def vis_pairgrids(data_day):\n",
    "    g = sns.PairGrid(data_day,\n",
    "                     x_vars= data_day.drop(columns = ['mw']).columns,\n",
    "                     y_vars=['mw'])\n",
    "\n",
    "    g = g.map(plt.scatter)\n",
    "    \n",
    "vis_pairgrids(get_data_day(data))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix\n",
    "\n",
    "Since the first method that will be explored is a simple linear regression, a pairwise grid comparing the electric demand `mw` to each feature in the data, and the different features to each other, is generated below. The correlation value plotted in each cell of the grid is the Pearson Correlation Coefficient value for the corresponding pair of features. By looking at this grid, we can see that there is no strong linear correlation between any of the features and the demand. However, some strong collinearity between features exists, as can be seen in the chart. This will potentially reduce the performance of the simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use data_day for the visualization\n",
    "data_day = get_data_day(data)\n",
    "vis_corr_matrix(data_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot pairwise charts to identify if any patterns exist. \n",
    "Due to the large number of feature columns, 4 features are plotted at a time by using subsets of the combined dataframe and the `vis_pairgrids` function. Features that are not numbers (such as sunrise time) are not plotted, as these values cannot be averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_subset_pairgrids(data):\n",
    "    mw = data['mw']\n",
    "    #dropping outcome mw and non-numeric and irrelevant features\n",
    "    all_features = data.drop(columns = ['mw'])\n",
    "    num_cols = len(all_features.columns)\n",
    "    #column subsets to make visualization clear\n",
    "    for subset in range(4, (num_cols//4*4 + 4*(num_cols % 4 != 0)) + 4, 4):\n",
    "        currFeatures = all_features.iloc[:, (subset - 4):subset]\n",
    "        currDF = pd.concat([mw, currFeatures], axis = 1)\n",
    "        currDF = get_data_day(currDF)\n",
    "        #visualizing pairgrid of 4 features at a time\n",
    "        vis_pairgrids(currDF)\n",
    "        \n",
    "    \n",
    "vis_subset_pairgrids(data.drop(columns = ['moonrise','moonset','sunrise','sunset']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the plots generated above, we can see definite polynomial trends in the temperature related terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solar Irradiance Data\n",
    "\n",
    "Lastly, we collect Solar irradiance data to observe any possible trends that may exist between irradiation and the energy demand. This is monthly aggregated data, based on historic data but not representative of separate moments in time. NREL's [Solar Resource Data API](https://developer.nrel.gov/docs/solar/solar-resource-v1/) is used to get this data. You can [get your own API key](https://developer.nrel.gov/signup/) and get free access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solar_irradiance(lat, lon, api_key = read_api_key('solar_api_key.txt')):\n",
    "    URL = 'https://developer.nrel.gov/api/solar/solar_resource/v1.json?lat=%s&lon=%s&api_key=%s&format=JSON'%(lat,lon,api_key)\n",
    "    response = requests.get(URL)\n",
    "    \n",
    "    jsonresp = json.loads(response.content)\n",
    "    return jsonresp\n",
    "\n",
    "solar_data = get_solar_irradiance('40.44','-79.99')\n",
    "\n",
    "months = ('jan','feb','march','apr','may','jun','jul','aug','sept','oct','nov','dec')\n",
    "dni = np.asarray(list(solar_data['outputs']['avg_dni']['monthly'].values()))\n",
    "ghi = np.asarray(list(solar_data['outputs']['avg_ghi']['monthly'].values()))\n",
    "lat_tilt = np.asarray(list(solar_data['outputs']['avg_lat_tilt']['monthly'].values()))\n",
    "\n",
    "data2 = data.copy()\n",
    "data2['month'] = [d.month for d in data.index]\n",
    "demand = data2.groupby('month')\n",
    "demand_vals = demand['mw'].agg(np.mean).to_numpy()\n",
    "\n",
    "demand_vals = (demand_vals - np.min(demand_vals))/(np.max(demand_vals) - np.min(demand_vals))\n",
    "dni = (dni - np.min(dni))/(np.max(dni) - np.min(dni))\n",
    "ghi = (ghi - np.min(ghi))/(np.max(ghi) - np.min(ghi))\n",
    "lat_tilt = (lat_tilt - np.min(lat_tilt))/(np.max(lat_tilt) - np.min(lat_tilt))\n",
    "\n",
    "\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.scatter(months,dni, label = 'Direct Normal Irradiance')\n",
    "plt.scatter(months,ghi, label = 'Global Horizontal Irradiance')\n",
    "plt.scatter(months,lat_tilt, label = 'Average Tilt at Latitude')\n",
    "plt.scatter(months,demand_vals, label = 'Energy Demand')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As depicted by the graph, there does not appear to be any visible trends so we will disregard this information moving forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Conversion and Pre-processing <a name = \"preprocess\"></a>\n",
    " \n",
    "As noticed from the pairwise plots above, some features do not behave linearly with respect to the outcome variable. For these features, a squared term is added.\n",
    "Other feature columns which do not make sense heuristically from the plots are dropped from the feature set as well. \n",
    "\n",
    "Other features which might be useful are added, namely:\n",
    "\n",
    "1) `is_weekend` - Boolean variable, 0 if weekday and 1 if weekend\n",
    "\n",
    "3) `month` - From date time \n",
    "\n",
    "4) `hour of day` - From date time \n",
    "\n",
    "2) `temprange` - Temperature range defined by max temp - min temp\n",
    "\n",
    "By splitting the data in this fashion, we will be able to group data in terms of \"weekend vs weekday\" or by month and compare values accordingly. \n",
    "\n",
    "The function below does this pre-processing. The prepared data is then stored in variable `pr_data`, which will be used in place of `data` for the rest of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepares data model for prediction\n",
    "def prepare_data(data):\n",
    "    #adding month, hour of day and is_weekend\n",
    "    data['month'] = [d.month for d in data.index]\n",
    "    data['hour_of_day'] = [d.hour for d in data.index]\n",
    "    data['day_of_week'] = [d.dayofweek for d in data.index]\n",
    "    #is_weekend\n",
    "    data['is_weekend'] = [0 for d in data.index]\n",
    "    data.loc[data['day_of_week'] > 4, 'is_weekend'] = 1\n",
    "    #adding temp_range\n",
    "    data['temp_range'] = data['maxtempC'] - data['mintempC']\n",
    "    #dropping columns    \n",
    "    dropped_columns = ['sunrise', 'sunset', 'moonrise', 'moonset', 'moon_illumination', 'winddirDegree','day_of_week']\n",
    "    data = data.drop(columns = dropped_columns)\n",
    "    \n",
    "    #squaring columns with strong but non linear trends\n",
    "    data['temp_range^2'] = data['temp_range']**2\n",
    "    data['maxtempC^2'] = data['maxtempC']**2\n",
    "    data['mintempC^2'] = data['mintempC']**2\n",
    "    data['DewPointC^2'] = data['DewPointC']**2\n",
    "    data['HeatIndexC^2'] = data['HeatIndexC']**2\n",
    "    data['WindChillC^2'] = data['WindChillC']**2\n",
    "    data['tempC^2'] = data['tempC']**2\n",
    "    \n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepared data\n",
    "pr_data = prepare_data(data)\n",
    "pr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairwise plots are now generated for the new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_subset_pairgrids(pr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the new plots, we can see that adding the squared term would better resemble a linear relationship. Higher degree terms are avoided to prevent overfitting. This is an instance of modeling a non-linear relationship/problem while still using linear regression methods. In some cases, for discrete variables like day of week and isweekend, these scatter plots are not meaningful.\n",
    "\n",
    "At the end of this pre-processing, we are left with a dataframe that contains only numeric values, ready to be converted as an array to be used for regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Input Data for Regression Methods <a name = \"tonumpy\"></a>\n",
    "\n",
    "We convert the pre-processed data into numpy arrays and split into X and y, where X and y are 32 bit floating point `np.ndarray` of all the features and the outcome variable (electric demand) respectively.\n",
    "The data is then standardized using sklearn's `MinMaxScaler()` method, to make the features comparable. It is then normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to standardize, then normalize feature data\n",
    "#called in get_Xy\n",
    "def standardize_data(X):\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    X = normalize(X)\n",
    "    return X\n",
    "\n",
    "#Returns arrays of features(X) and demand values(y)\n",
    "def get_Xy(data):\n",
    "    X = data.drop(columns = ['mw'])\n",
    "    X = X.to_numpy(dtype = np.float32)\n",
    "    X = standardize_data(X)\n",
    "    \n",
    "    y = data[\"mw\"].to_numpy(dtype = np.float32)\n",
    "    return(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting prepared data as numpy ndarrays\n",
    "X , y = get_Xy(pr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below splits given data into test and train sets for any given test size. This function is used later, once the model is defined, to explore the optimum split size for this data. Currently, a default value of 20% for test data size is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns train and test data in format (x_train, x_test, y_train, y_test)\n",
    "#Splitting data - not shuffling, since it is a time series problem\n",
    "def split_data(X,y,test_size = 0.2):\n",
    "    #https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "    return train_test_split(X,y,test_size = test_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection using Orthogonal Matching Pursuit <a name = \"omp\"></a>\n",
    "\n",
    "Orthogonal Matching Pursuit (OMP) was utilized to select the top \"k\" number of features in a specific regression instance. \n",
    "*\"OMP tends to select only one from correlated features, because the next selected feature relies on a residual that is orthogonal to previous selected features.\"*<sup>[1]</sup>\n",
    "OMP can be visualized in terms of signals. It takes a target signal (y, in this case) and tries to best fit the feature \"signals\" to the target given certain constraints. \n",
    "\n",
    "Sklearn's `OrthogonalMatchingPursuit` class can be used to create an OMP object, which can then be used to `fit` the training X and y, and get coefficients that allow us to determine whether a feature should be retained or not. The OMP object is initialized using a certain number of non-zero coefficients, **k**. The features which have the non-zero coefficients are the top k features. \n",
    "\n",
    "The functions defined below are used later in the project to perform the feature selection given some k, and a new feature array which contains only the top 'k' feature values for all hours is returned. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for get_top_N_features\n",
    "def X_after_OMP(X, dropped_indices):\n",
    "    X = np.delete(X, dropped_indices, axis = 1)\n",
    "    return X\n",
    "\n",
    "#returns top N features using OMP \n",
    "def get_top_N_features(N, X_train, X, y_train):\n",
    "    \n",
    "    #initializing model\n",
    "    omp_model = OrthogonalMatchingPursuit(n_nonzero_coefs = N)\n",
    "    \n",
    "    #fit using training data\n",
    "    omp_model.fit(X_train, y_train)\n",
    "    dropped_indices = []\n",
    "    feature_idx_coef = collections.defaultdict(list)\n",
    "    coefs = omp_model.coef_\n",
    "    \n",
    "    #storing indices\n",
    "    for i, coef in enumerate(coefs):\n",
    "        if coef == 0:\n",
    "            dropped_indices.append(i)\n",
    "        else:\n",
    "            feature_idx_coef[i] = coef\n",
    "    #getting new feature array with only top 'k' features\n",
    "    X_new = X_after_OMP(X, dropped_indices)\n",
    "\n",
    "    return (X_new, coefs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how this function works, we can look at a specific example. Say, we want to get the top 5 features. We can call this function with the argument N and the training data to get the top 5 features. With a little tweaking with the dataframe, we can also get the names of these features. We can see that OMP returns the top 5 features (not in order of importance, but in the order in which they are stored in the array/dataframe) to be `maxtempC`, `hour_of_day`,`is_weekend`,`DewPointC^2` and `WindChillC^2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporary, for illustration\n",
    "ex_X_train, ex_X_test, ex_y_train, ex_y_test = split_data(X,y)\n",
    "X_new, coefs = get_top_N_features(5 ,ex_X_train, X, ex_y_train)\n",
    "coef_df = pd.DataFrame({'feature' : pr_data.columns.values[1:],\n",
    "                        'coef' : list(coefs)})\n",
    "coef_df = coef_df[coef_df['coef'] != 0]\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand how the feature selection works, we can move on to assessing model performance for different regression models for a varying number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis -  Simple regression, Random Forest regression and Bagging Regression <a name = \"regression\"></a>\n",
    "\n",
    "### Preliminary Analysis <a name = \"prelim\"></a>\n",
    "In this section of the project, sklearn is used to fit different regression models - Simple Regression, Random Forest Regression and Bagging Regression - to the training data. The Models are evaluated using MSE (Mean Squared Error) and R-squared ($R^2$) for each of the models.\n",
    "\n",
    "This comparison is done across a varying number of features, right from using a single feature, to all possible features. The MSE is then plotted below for parametrically varying number of features. To do the parametric runs, the functions below are utilized. `performance_analysis` takes X, y, a test size, and string input for type of regression model, and returns a tuple of two lists containing $R^2$ values and MSE values for that specific model, respectively.\n",
    "\n",
    "The model parameter is a string, which can be 'lr' (linear regressor), 'br' (bagging regressor) or 'rf' (random forest regressor). For bagging regressor, the default base estimator, that is decision tree, was retained.\n",
    "\n",
    "Some default input values used in the function are discussed later in this sub-section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for performance_analysis\n",
    "#returns R^2 and MSE values\n",
    "def get_prediction_performance(y_target, y_pred):\n",
    "    rsq = r2_score(y_target, y_pred)\n",
    "    mse = mean_squared_error(y_target, y_pred)\n",
    "    return rsq, mse\n",
    "\n",
    "def performance_analysis(X, y, test_size, model):\n",
    "    #splitting into training and testing parts\n",
    "    Xtr,Xte,Ytr,Yte = split_data(X,y, test_size)\n",
    "    #initializing empty lists for R^2 and MSE values\n",
    "    rsq_vals= []\n",
    "    mse_vals= []\n",
    "    \n",
    "    #Parametrically varying number of features\n",
    "    for i in tqdm(range(1,X.shape[1])):\n",
    "        X_new, coefs = get_top_N_features(i ,Xtr, X, Ytr)\n",
    "        X_train, X_test, y_train, y_test = split_data(X_new,y, test_size)\n",
    "        \n",
    "        #Fitting model based on 'model' parameter\n",
    "        #linear regressor\n",
    "        if model == 'lr':\n",
    "            currModel = LinearRegression()\n",
    "            currModel.fit(X_train, y_train)\n",
    "        #random forest regressor\n",
    "        elif model == 'rf':\n",
    "            currModel = RandomForestRegressor(n_estimators = 100)\n",
    "            currModel.fit(X_train, y_train)\n",
    "        #bagging regressor\n",
    "        elif model == \"br\":\n",
    "            #default base estimator - decision tree. Retained\n",
    "            currModel = BaggingRegressor(n_estimators = 100)\n",
    "            currModel.fit(X_train, y_train)\n",
    "            \n",
    "        #getting predicted values\n",
    "        y_pred = currModel.predict(X_test)\n",
    "        rsq, mse= get_prediction_performance(y_test, y_pred)\n",
    "        rsq_vals.append(rsq)\n",
    "        mse_vals.append(mse)\n",
    "    \n",
    "    return (rsq_vals, mse_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test size selection\n",
    "\n",
    "One of the important input parameters for the function above is the test size. To check which training data size would give us the best performance, several input proportions for test size were tested, between 20% and 35%. The code block below evaluates what test_size should be passed to the function for more accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arriving at test_size\n",
    "test_size = [0.20,0.25,0.26,0.27,0.28,0.29,0.30,0.31,0.32,0.33,0.34,0.35]\n",
    "\n",
    "ideal = 0\n",
    "max_r2 = 0\n",
    "for i in test_size:\n",
    "    rsq_vals, mse_vals = performance_analysis(X,y,i,'lr')\n",
    "    maxval = np.amax(rsq_vals)\n",
    "    if (maxval > max_r2):\n",
    "        max_r2 = maxval\n",
    "        ideal = i\n",
    "\n",
    "print(\"ideal test size: \" , ideal)\n",
    "print(\"maximum R square: \", max_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, we proceed with 29% of testing data and 71% training data. Since we are comparing the three models to each other, the same proportion is used for all three models. \n",
    "\n",
    "#### Number of Estimators for Random Forest\n",
    "\n",
    "The default number of estimators (`n_estimators`) is 10 (or 100 for v 0.22). Using more estimators can increase the time required to run the model. The following code block was used to determine the approximately appropriate number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = split_data(X,y,0.29)\n",
    "#arriving at n_estimators\n",
    "res_dict = collections.defaultdict(list)\n",
    "for n_estimators in tqdm([10,20,50,100,150,200]):\n",
    "    t1 = time.time()\n",
    "    curr_rf = RandomForestRegressor(n_estimators = n_estimators)\n",
    "    curr_rf.fit(X_train_rf, y_train_rf)\n",
    "    y_pred_rf = curr_rf.predict(X_test_rf)\n",
    "    t2 = time.time()\n",
    "    print(\"For %d decision trees : \"%(n_estimators))\n",
    "    tt = t2 - t1\n",
    "    res_dict['time'].append(tt)\n",
    "    print(\"Time taken : \", tt)\n",
    "    pp = get_prediction_performance(y_test_rf, y_pred_rf)\n",
    "    print(\"R^2 : \", pp[0])\n",
    "    res_dict['r2'].append(pp[0])\n",
    "    print(\"MSE : \",pp[1], \"\\n\")\n",
    "    res_dict['mse'].append(pp[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the change in R-squared with increasing number of estimators first, to check the impact of more estimators. The R-squared shows an increase in trend, but the difference in these values is very marginal. The maximum possible $R^2$ value is a 1.2 percentage point increase in the degree to which the regression model can explain variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([10,20,50,100,150,200], res_dict['r2'], label = 'R squares')\n",
    "plt.xlabel('Number of estimators')\n",
    "# plt.ylabel('Time Taken')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(max(res_dict['r2']) - min(res_dict['r2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the normalized values for time taken, $R^2$ and MSE were plotted together. From both these charts, we can conclude that the new default 100 is the best possible value for the number of estimators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_norm = np.asarray(res_dict['time'])\n",
    "time_norm = (time_norm - np.min(time_norm))/(np.max(time_norm) - np.min(time_norm))\n",
    "\n",
    "mse_norm = np.asarray(res_dict['mse'])\n",
    "mse_norm = (mse_norm - np.min(mse_norm)) /(np.max(mse_norm) - np.min(mse_norm))\n",
    "\n",
    "r2_norm = np.asarray(res_dict['r2'])\n",
    "r2_norm = (r2_norm - np.min(r2_norm))/(np.max(r2_norm) - np.min(r2_norm))\n",
    "plt.plot([10,20,50,100,150,200], time_norm, label = 'Time Taken norm')\n",
    "plt.plot([10,20,50,100,150,200], r2_norm, label = 'R-Squared')\n",
    "plt.plot([10,20,50,100,150,200], mse_norm, label = \"MSE norm\")\n",
    "plt.xlabel('Number of estimators')\n",
    "# plt.ylabel('Time Taken')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar analysis was conducted to check the impact of number of estimators on the bagging regression performance. The code below runs this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_br, X_test_br, y_train_br, y_test_br = split_data(X,y,0.29)\n",
    "#arriving at n_estimators\n",
    "res_dict_br = collections.defaultdict(list)\n",
    "for n_estimators in tqdm([10,20,50,100,150]):\n",
    "    t1 = time.time()\n",
    "    curr_br = BaggingRegressor(n_estimators = n_estimators)\n",
    "    curr_br.fit(X_train_br, y_train_br)\n",
    "    y_pred_br = curr_br.predict(X_test_br)\n",
    "    t2 = time.time()\n",
    "    \n",
    "    print(\"For %d decision trees : \"%(n_estimators))\n",
    "    tt = t2 - t1\n",
    "    res_dict_br['time'].append(tt)\n",
    "    print(\"Time taken : \", tt)\n",
    "    \n",
    "    pp = get_prediction_performance(y_test_br, y_pred_br)\n",
    "    print(\"R^2 : \", pp[0])\n",
    "    res_dict_br['r2'].append(pp[0])\n",
    "    print(\"MSE : \",pp[1], \"\\n\")\n",
    "    res_dict_br['mse'].append(pp[1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar normalized plot is generated for the bagging regressor. From this chart, we can see that we stop seeing steep benefits when the number of estimators is about 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_norm = np.asarray(res_dict_br['time'])\n",
    "time_norm = (time_norm - np.min(time_norm))/(np.max(time_norm) - np.min(time_norm))\n",
    "\n",
    "mse_norm = np.asarray(res_dict_br['mse'])\n",
    "mse_norm = (mse_norm - np.min(mse_norm)) /(np.max(mse_norm) - np.min(mse_norm))\n",
    "\n",
    "r2_norm = np.asarray(res_dict_br['r2'])\n",
    "r2_norm = (r2_norm - np.min(r2_norm))/(np.max(r2_norm) - np.min(r2_norm))\n",
    "\n",
    "#generating plot\n",
    "plt.plot([10,20,50,100,150], time_norm, label = 'Time Taken norm')\n",
    "plt.plot([10,20,50,100,150], r2_norm, label = 'R-Squared')\n",
    "plt.plot([10,20,50,100,150], mse_norm, label = \"MSE norm\")\n",
    "plt.xlabel('Number of estimators')\n",
    "# plt.ylabel('Time Taken')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison <a name = \"comparison\"></a>\n",
    "\n",
    "Using the functions defined above, models were fit and evaluated for varying top 'k' features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iteratively running models for 1 to 30 features\n",
    "#Linear regression\n",
    "rsq_vals_lr, mse_vals_lr = performance_analysis(X,y,0.29,\"lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest\n",
    "rsq_vals_rf, mse_vals_rf = performance_analysis(X,y,0.29,\"rf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bagging regression\n",
    "rsq_vals_br, mse_vals_br = performance_analysis(X,y,0.29,\"br\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting MSE values for all three models for varying number of predictors\n",
    "num_features= list(range(1,X.shape[1]))\n",
    "plt.plot(num_features, mse_vals_lr, label = \"Linear Regression MSE\")\n",
    "plt.plot(num_features, mse_vals_rf, label = \"Random Forest MSE\")\n",
    "plt.plot(num_features, mse_vals_br, label = \"Bagging Regressor MSE\")\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(rsq_vals_lr)):\n",
    "#     print(str(i+1) + \" : \" + str(rsq_vals_lr[i]) + str(rsq_vals_rf[i]) + str(rsq_vals_br[i]))\n",
    "plt.plot(num_features, rsq_vals_lr, label = \"Linear Regression R^2\")\n",
    "plt.plot(num_features, rsq_vals_rf, label = \"Random Forest R^2\")\n",
    "plt.plot(num_features, rsq_vals_br, label = \"Bagging Regressor R^2\")\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the charts generated above, we can see that the performance improvement plateaus at around 6 features. The ensemble methods perform much better with these features than the simple regression model.\n",
    "\n",
    "The performance of the bagging regressor and random forest regressor are very similar, but random forest regressor takes longer to run. Therefore, from this analysis, a bagging regressor with the top 6 features would be ideal.\n",
    "The predicted and observed values are plotted below. A dataframe with the top 6 features (not in order) is also shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_br, X_test_br, y_train_br, y_test_br = split_data(X,y,0.29)\n",
    "X_new_br, coefs = get_top_N_features(6 ,X_train_br, X, y_train_br)\n",
    "X_train_br, X_test_br, y_train_br, y_test_br = split_data(X_new_br, y, 0.29)\n",
    "br_model = BaggingRegressor(n_estimators = 100)\n",
    "br_model.fit(X_train_br, y_train_br)\n",
    "\n",
    "y_pred_br = br_model.predict(X_test_br)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting predicted and observed values\n",
    "x = np.arange(0,len(y_test_br))\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.scatter(x,y_pred_br,alpha = 0.5, label = 'Predicted Values')\n",
    "plt.scatter(x,y_test_br, alpha = 0.5, label = 'Observed Test Values')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing top 6 features \n",
    "coef_df = pd.DataFrame({'feature' : pr_data.columns.values[1:],\n",
    "                        'coef' : list(coefs)})\n",
    "coef_df = coef_df[coef_df['coef'] != 0]\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Forecast with PJM Forecast Values <a name = 'forecastcompare'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #X_train = hist_weather = get_wwohist_weather(['pittsburgh'], '01-MAY-2016','10-DEC-2019',1)\n",
    "# X_train = pd.read_csv('pittsburgh.csv')\n",
    "# y_train = pd.read_csv(\"y_train.csv\")\n",
    "\n",
    "# print(y_train)\n",
    "\n",
    "# data = get_merged_load_weather(y_train,X_train)\n",
    "# data = prepare_data(data)\n",
    "\n",
    "\n",
    "# X = data[['maxtempC', 'hour_of_day', 'is_weekend', 'DewPointC^2','HeatIndexC^2','WindChillC^2']]\n",
    "# y = data[\"mw\"].to_numpy(dtype = np.float32)\n",
    "# br_model = BaggingRegressor(n_estimators = 50)\n",
    "# br_model.fit(X, y)\n",
    "\n",
    "# # performance_analysis(X_new,y)\n",
    "\n",
    "\n",
    "# y_test = pd.read_csv(\"y_test.csv\")['forecast_load_mw'].dropna()\n",
    "# X_test = pd.read_csv(\"X_test.csv\")\n",
    "\n",
    "# X_test.to_numpy(dtype = np.float32)\n",
    "# X_test = standardize_data(X_test)\n",
    "\n",
    "# y_pred = br_model.predict(X_test)\n",
    "# rsq,mse = get_prediction_performance(y_test,y_pred)\n",
    "# print(rsq)\n",
    "# print(mse)\n",
    "\n",
    "\n",
    "# #Plotting predicted and observed values\n",
    "# x = np.arange(0,len(y_test))\n",
    "# plt.figure(figsize = (20,8))\n",
    "# plt.scatter(x,y_pred, alpha = 0.5, label = 'Predicted Values')\n",
    "# plt.scatter(x,y_test, alpha = 0.5, label = 'Observed Test Values')\n",
    "# plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Solar Irradiance and  Electric Demand\n",
    "\n",
    "The solar data(you can see it printed in the data collection section) has average monthly solar irradiance values. These values are aggregated monthly values over 12 years or so. \n",
    "\n",
    "Maybe plot the solar monthly values and the electric demand aggregated by month on the same plot to check if it has any impact. I would normalize before I do it because the values are so big!\n",
    "\n",
    "You could also incorporate it as a feature and check its importance (upto you, your call)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1) [1] Shi, Xiaoshuang, et al. \"Structured orthogonal matching pursuit for feature selection.\" Neurocomputing 349 (2019): 164-172.\n",
    "\n",
    "2) Scikit-learn documentation for regression, OMP and accuracy metrics:  https://scikit-learn.org/\n",
    "\n",
    "3) PJM DataMiner2 Documentation\n",
    "\n",
    "4) wwo-hist documentation: https://pypi.org/project/wwo-hist/\n",
    "\n",
    "5) Seaborn Official Documentation: https://seaborn.pydata.org/examples/many_pairwise_correlations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
